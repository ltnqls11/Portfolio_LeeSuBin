# 1. ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘ (í¬ë¡¤ë§)
import requests
from bs4 import BeautifulSoup
import urllib.parse
import time

def get_naver_news_api(query, page=1):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ API ì‚¬ìš© (ë” ì•ˆì •ì )"""
    try:
        # ë„¤ì´ë²„ ê°œë°œì ì„¼í„°ì—ì„œ ë°œê¸‰ë°›ì€ API í‚¤ê°€ í•„ìš”í•˜ì§€ë§Œ, 
        # ì—¬ê¸°ì„œëŠ” RSS í”¼ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì•ˆ ë°©ë²•ì„ ì‹œë„
        print(f"ğŸ” ë„¤ì´ë²„ ë‰´ìŠ¤ RSS ê²€ìƒ‰ ì¤‘: {query}")
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ RSS í”¼ë“œ URL
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.naver.com/main/list.naver?mode=LSD&mid=sec&sid1=105&listType=title&date=20240724"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml'
        }
        
        # RSS íŒŒì‹± ì‹œë„
        try:
            import feedparser
            feed = feedparser.parse(rss_url)
            
            articles = []
            for entry in feed.entries[:10]:
                articles.append({
                    'title': entry.title,
                    'link': entry.link
                })
            
            if articles:
                print(f"âœ… RSSë¡œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                return articles
                
        except ImportError:
            print("âš ï¸ feedparser ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ RSS íŒŒì‹± ì˜¤ë¥˜: {e}")
        
        return []
        
    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ë‰´ìŠ¤ API ì˜¤ë¥˜: {e}")
        return []

def get_alternative_news_sources(query):
    """ëŒ€ì•ˆ ë‰´ìŠ¤ ì†ŒìŠ¤ë“¤ ì‹œë„"""
    articles = []
    
    # 1. ë‹¤ìŒ ë‰´ìŠ¤ ì‹œë„
    try:
        print(f"ğŸ” ë‹¤ìŒ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„: {query}")
        encoded_query = urllib.parse.quote(query)
        daum_url = f"https://search.daum.net/search?w=news&q={encoded_query}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'
        }
        
        response = requests.get(daum_url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë‹¤ìŒ ë‰´ìŠ¤ êµ¬ì¡°ì— ë§ëŠ” ì…€ë ‰í„°
            news_items = soup.select('.c-item-doc')
            
            for item in news_items[:5]:
                try:
                    title_elem = item.select_one('.tit-g')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        link = title_elem.get('href', '')
                        
                        if title and link:
                            articles.append({
                                'title': title,
                                'link': link
                            })
                except:
                    continue
            
            if articles:
                print(f"âœ… ë‹¤ìŒ ë‰´ìŠ¤ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                return articles
                
    except Exception as e:
        print(f"âš ï¸ ë‹¤ìŒ ë‰´ìŠ¤ ì˜¤ë¥˜: {e}")
    
    # 2. êµ¬ê¸€ ë‰´ìŠ¤ ì‹œë„
    try:
        print(f"ğŸ” êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œë„: {query}")
        encoded_query = urllib.parse.quote(f"{query} site:news.naver.com OR site:news.joins.com OR site:news.chosun.com")
        google_url = f"https://www.google.com/search?q={encoded_query}&tbm=nws&hl=ko"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'ko-KR,ko;q=0.9'
        }
        
        response = requests.get(google_url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # êµ¬ê¸€ ë‰´ìŠ¤ ê²°ê³¼ íŒŒì‹±
            news_items = soup.select('div[data-ved]')
            
            for item in news_items[:5]:
                try:
                    title_elem = item.select_one('h3')
                    link_elem = item.select_one('a')
                    
                    if title_elem and link_elem:
                        title = title_elem.get_text(strip=True)
                        link = link_elem.get('href', '')
                        
                        if title and link and 'http' in link:
                            articles.append({
                                'title': title,
                                'link': link
                            })
                except:
                    continue
            
            if articles:
                print(f"âœ… êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                return articles
                
    except Exception as e:
        print(f"âš ï¸ êµ¬ê¸€ ë‰´ìŠ¤ ì˜¤ë¥˜: {e}")
    
    return articles

def get_naver_news(query, page=1):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (ìš°íšŒ ë°©ë²• í¬í•¨)"""
    print(f"ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: {query}")
    
    # ë°©ë²• 1: ëŒ€ì•ˆ ë‰´ìŠ¤ ì†ŒìŠ¤ ì‹œë„
    articles = get_alternative_news_sources(query)
    if articles:
        return articles
    
    # ë°©ë²• 2: ë„¤ì´ë²„ ë‰´ìŠ¤ API/RSS ì‹œë„
    articles = get_naver_news_api(query, page)
    if articles:
        return articles
    
    # ë°©ë²• 3: ê°„ë‹¨í•œ ìš°íšŒ ì‹œë„ (User-Agent ë³€ê²½, ë”œë ˆì´ ì¶”ê°€)
    try:
        print("ğŸ”„ ìš°íšŒ ë°©ë²•ìœ¼ë¡œ ë„¤ì´ë²„ ë‰´ìŠ¤ ì‹œë„...")
        
        # ë‹¤ì–‘í•œ User-Agent ì‹œë„
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:91.0) Gecko/20100101 Firefox/91.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36'
        ]
        
        for ua in user_agents:
            try:
                time.sleep(2)  # ë”œë ˆì´ ì¶”ê°€
                
                encoded_query = urllib.parse.quote(query)
                url = f"https://search.naver.com/search.naver?where=news&query={encoded_query}"
                
                headers = {
                    'User-Agent': ua,
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache'
                }
                
                response = requests.get(url, headers=headers, timeout=15)
                
                if response.status_code == 200 and 'ì°¨ë‹¨' not in response.text:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°
                    links = soup.find_all('a', href=True)
                    articles = []
                    
                    for link in links:
                        href = link.get('href', '')
                        title = link.get_text(strip=True)
                        
                        # ë‰´ìŠ¤ ë§í¬ í•„í„°ë§
                        if ('news.naver.com' in href or 'n.news.naver.com' in href) and len(title) > 10:
                            articles.append({
                                'title': title,
                                'link': href
                            })
                    
                    if articles:
                        print(f"âœ… ìš°íšŒ ë°©ë²•ìœ¼ë¡œ {len(articles[:5])}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                        return articles[:5]
                        
            except Exception as e:
                print(f"âš ï¸ User-Agent {ua[:20]}... ì‹¤íŒ¨: {e}")
                continue
                
    except Exception as e:
        print(f"âŒ ìš°íšŒ ì‹œë„ ì‹¤íŒ¨: {e}")
    
    print("âš ï¸ ëª¨ë“  ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ë°©ë²• ì‹¤íŒ¨")
    return []

# ëŒ€ì•ˆ ë‰´ìŠ¤ ì†ŒìŠ¤ ì¶”ê°€
def get_sample_news():
    """ìƒ˜í”Œ ë‰´ìŠ¤ ë°ì´í„° (ë„¤ì´ë²„ ì ‘ê·¼ ë¶ˆê°€ì‹œ ì‚¬ìš©)"""
    return [
        {
            'title': 'ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ì‹œì¥, AI ê¸°ìˆ ë¡œ ê¸‰ì„±ì¥',
            'link': 'https://example.com/news1',
            'content': 'ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì„ í™œìš©í•œ ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ì‹œì¥ì´ ê¸‰ì†ë„ë¡œ ì„±ì¥í•˜ê³  ìˆë‹¤. ì›ê²© ì§„ë£Œ, ì›¨ì–´ëŸ¬ë¸” ë””ë°”ì´ìŠ¤, ê±´ê°• ëª¨ë‹ˆí„°ë§ ì•± ë“±ì´ ì£¼ìš” ì„±ì¥ ë™ë ¥ì´ ë˜ê³  ìˆìœ¼ë©°, íŠ¹íˆ ì½”ë¡œë‚˜19 ì´í›„ ë¹„ëŒ€ë©´ ì˜ë£Œ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ìˆ˜ìš”ê°€ í¬ê²Œ ì¦ê°€í–ˆë‹¤. ì „ë¬¸ê°€ë“¤ì€ í–¥í›„ 5ë…„ê°„ ì—°í‰ê·  25% ì´ìƒì˜ ì„±ì¥ë¥ ì„ ë³´ì¼ ê²ƒìœ¼ë¡œ ì „ë§í•œë‹¤ê³  ë°í˜”ë‹¤.'
        },
        {
            'title': 'ì›¨ì–´ëŸ¬ë¸” ë””ë°”ì´ìŠ¤ë¡œ ê±´ê°• ê´€ë¦¬ í˜ì‹ ',
            'link': 'https://example.com/news2', 
            'content': 'ìŠ¤ë§ˆíŠ¸ì›Œì¹˜ì™€ í”¼íŠ¸ë‹ˆìŠ¤ íŠ¸ë˜ì»¤ ë“± ì›¨ì–´ëŸ¬ë¸” ë””ë°”ì´ìŠ¤ê°€ ê°œì¸ ê±´ê°• ê´€ë¦¬ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•˜ê³  ìˆë‹¤. ì‹¤ì‹œê°„ ì‹¬ë°•ìˆ˜ ëª¨ë‹ˆí„°ë§, ìˆ˜ë©´ íŒ¨í„´ ë¶„ì„, ìš´ë™ëŸ‰ ì¸¡ì • ë“±ì˜ ê¸°ëŠ¥ì„ í†µí•´ ì‚¬ìš©ìë“¤ì´ ìì‹ ì˜ ê±´ê°• ìƒíƒœë¥¼ ì§€ì†ì ìœ¼ë¡œ ê´€ì°°í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤. ì˜ë£Œì§„ë“¤ë„ ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ í™œìš©í•´ ë” ì •í™•í•œ ì§„ë‹¨ê³¼ ì¹˜ë£Œ ê³„íšì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆë‹¤.'
        },
        {
            'title': 'ì›ê²© ì§„ë£Œ í”Œë«í¼ í™•ì‚°ìœ¼ë¡œ ì˜ë£Œ ì ‘ê·¼ì„± í–¥ìƒ',
            'link': 'https://example.com/news3',
            'content': 'ì½”ë¡œë‚˜19 íŒ¬ë°ë¯¹ì„ ê³„ê¸°ë¡œ ì›ê²© ì§„ë£Œ ì„œë¹„ìŠ¤ê°€ ë³¸ê²©í™”ë˜ë©´ì„œ ì˜ë£Œ ì ‘ê·¼ì„±ì´ í¬ê²Œ í–¥ìƒë˜ê³  ìˆë‹¤. íŠ¹íˆ ê±°ë™ì´ ë¶ˆí¸í•œ í™˜ìë‚˜ ì§€ë°© ê±°ì£¼ìë“¤ì—ê²Œ í° ë„ì›€ì´ ë˜ê³  ìˆìœ¼ë©°, ì˜ë£Œì§„ ì—­ì‹œ íš¨ìœ¨ì ì¸ í™˜ì ê´€ë¦¬ê°€ ê°€ëŠ¥í•´ì¡Œë‹¤. ì •ë¶€ëŠ” ì›ê²© ì§„ë£Œ ê´€ë ¨ ê·œì œë¥¼ ì™„í™”í•˜ê³  ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ìƒíƒœê³„ ì¡°ì„±ì— ì ê·¹ ë‚˜ì„œê³  ìˆë‹¤.'
        },
        {
            'title': 'AI ì§„ë‹¨ ì‹œìŠ¤í…œ, ì˜ë£Œì§„ ì—…ë¬´ íš¨ìœ¨ì„± í¬ê²Œ í–¥ìƒ',
            'link': 'https://example.com/news4',
            'content': 'ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ì˜ë£Œ ì§„ë‹¨ ì‹œìŠ¤í…œì´ ì˜ë£Œì§„ì˜ ì—…ë¬´ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ê³  ìˆë‹¤. ì˜ìƒ íŒë…, ë³‘ë¦¬ ì§„ë‹¨, ì•½ë¬¼ ì²˜ë°© ë“± ë‹¤ì–‘í•œ ì˜ì—­ì—ì„œ AIê°€ í™œìš©ë˜ë©´ì„œ ì§„ë‹¨ ì •í™•ë„ê°€ ë†’ì•„ì§€ê³  ì˜ë£Œ ì˜¤ë¥˜ê°€ ê°ì†Œí•˜ê³  ìˆë‹¤. íŠ¹íˆ í¬ê·€ ì§ˆí™˜ ì§„ë‹¨ì—ì„œ AIì˜ ë„ì›€ìœ¼ë¡œ ì¡°ê¸° ë°œê²¬ìœ¨ì´ 30% ì´ìƒ ì¦ê°€í•œ ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ë‹¤.'
        },
        {
            'title': 'ë””ì§€í„¸ ì¹˜ë£Œì œ ì‹œì¥ ê¸‰ì„±ì¥, ìƒˆë¡œìš´ ì¹˜ë£Œ íŒ¨ëŸ¬ë‹¤ì„ ì œì‹œ',
            'link': 'https://example.com/news5',
            'content': 'ì•± ê¸°ë°˜ ë””ì§€í„¸ ì¹˜ë£Œì œê°€ ì „í†µì ì¸ ì•½ë¬¼ ì¹˜ë£Œì˜ ëŒ€ì•ˆìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆë‹¤. ì •ì‹ ê±´ê°•, ì¤‘ë… ì¹˜ë£Œ, ë§Œì„±ì§ˆí™˜ ê´€ë¦¬ ë“±ì˜ ë¶„ì•¼ì—ì„œ ë””ì§€í„¸ ì¹˜ë£Œì œì˜ íš¨ê³¼ê°€ ì…ì¦ë˜ë©´ì„œ ì‹œì¥ ê·œëª¨ê°€ ê¸‰ì†íˆ í™•ëŒ€ë˜ê³  ìˆë‹¤. FDA ìŠ¹ì¸ì„ ë°›ì€ ë””ì§€í„¸ ì¹˜ë£Œì œë„ ëŠ˜ì–´ë‚˜ê³  ìˆì–´ í–¥í›„ ì˜ë£Œ íŒ¨ëŸ¬ë‹¤ì„ì˜ ë³€í™”ê°€ ì˜ˆìƒëœë‹¤.'
        }
    ]

def get_google_news(query):
    """êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ (ëŒ€ì•ˆ)"""
    try:
        import feedparser
        
        # êµ¬ê¸€ ë‰´ìŠ¤ RSS í”¼ë“œ ì‚¬ìš©
        encoded_query = urllib.parse.quote(query)
        rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
        
        print(f"ğŸ” êµ¬ê¸€ ë‰´ìŠ¤ RSS ê²€ìƒ‰ ì¤‘: {query}")
        
        feed = feedparser.parse(rss_url)
        articles = []
        
        for entry in feed.entries[:10]:  # ìµœëŒ€ 10ê°œ
            articles.append({
                'title': entry.title,
                'link': entry.link,
                'published': entry.get('published', ''),
                'summary': entry.get('summary', '')
            })
        
        print(f"âœ… êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
        return articles
        
    except ImportError:
        print("âš ï¸ feedparserê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (pip install feedparser)")
        return []
    except Exception as e:
        print(f"âŒ êµ¬ê¸€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return []

print("ğŸš€ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")

# 1ì°¨ ì‹œë„: ë„¤ì´ë²„ ë‰´ìŠ¤
articles = get_naver_news("ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´", page=1)

# 2ì°¨ ì‹œë„: êµ¬ê¸€ ë‰´ìŠ¤ (ë„¤ì´ë²„ ì‹¤íŒ¨ì‹œ)
if not articles:
    print("âš ï¸ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨. êµ¬ê¸€ ë‰´ìŠ¤ë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
    articles = get_google_news("ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´")

# 3ì°¨ ì‹œë„: ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© (ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ì‹œ)
if not articles:
    print("âš ï¸ ì˜¨ë¼ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨. ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤...")
    sample_articles = get_sample_news()
    
    # ìƒ˜í”Œ ë°ì´í„°ë¥¼ ê¸°ì¡´ í˜•ì‹ì— ë§ê²Œ ë³€í™˜
    articles = []
    for sample in sample_articles:
        articles.append({
            'title': sample['title'],
            'link': sample['link']
        })
    
    print(f"âœ… ìƒ˜í”Œ ë°ì´í„° {len(articles)}ê°œ ë¡œë“œ ì™„ë£Œ")

if not articles:
    print("âŒ ëª¨ë“  ë°©ë²•ìœ¼ë¡œ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    exit()

print(f"ğŸ‰ ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì„±ê³µ!")

# 2. ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘
def extract_article_text(url):
    """ê¸°ì‚¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        print(f"ğŸ“„ ë³¸ë¬¸ ì¶”ì¶œ ì¤‘: {url[:50]}...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
        }
        
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
        
        soup = BeautifulSoup(res.text, 'html.parser')
        
        # ë‹¤ì–‘í•œ ë³¸ë¬¸ ì…€ë ‰í„° ì‹œë„ (ë§¤ì²´ë³„ë¡œ ë‹¤ë¦„)
        content_selectors = [
            'div#dic_area',           # ë„¤ì´ë²„ ë‰´ìŠ¤
            'div.newsct_article',     # ë„¤ì´ë²„ ë‰´ìŠ¤ (ìƒˆ êµ¬ì¡°)
            'div#articleBodyContents', # ë„¤ì´ë²„ ë‰´ìŠ¤ (êµ¬ êµ¬ì¡°)
            'article',                # ì¼ë°˜ì ì¸ article íƒœê·¸
            'div.article_body',       # ì¼ë¶€ ì–¸ë¡ ì‚¬
            'div.news_body',          # ì¼ë¶€ ì–¸ë¡ ì‚¬
            'div.article-body',       # ì¼ë¶€ ì–¸ë¡ ì‚¬
            'div.content',            # ì¼ë°˜ì ì¸ content
            'div.post-content',       # ë¸”ë¡œê·¸í˜•
            'div.entry-content'       # ì›Œë“œí”„ë ˆìŠ¤í˜•
        ]
        
        content_div = None
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                print(f"âœ… ë³¸ë¬¸ ë°œê²¬: {selector}")
                break
        
        if not content_div:
            # ë§ˆì§€ë§‰ ì‹œë„: p íƒœê·¸ë“¤ ìˆ˜ì§‘
            paragraphs = soup.find_all('p')
            if paragraphs:
                text = ' '.join([p.get_text(strip=True) for p in paragraphs])
                print(f"âœ… p íƒœê·¸ë¡œ ë³¸ë¬¸ ì¶”ì¶œ ({len(text)} ë¬¸ì)")
                return text[:2000]  # ìµœëŒ€ 2000ì
            else:
                print("âš ï¸ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return ''
        
        # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
        for unwanted in content_div.find_all(['script', 'style', 'iframe', 'ins', 'aside']):
            unwanted.decompose()
        
        text = content_div.get_text(separator=' ', strip=True)
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        text = ' '.join(text.split())  # ê³µë°± ì •ë¦¬
        text = text.replace('\n', ' ').replace('\t', ' ')
        
        print(f"âœ… ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ ({len(text)} ë¬¸ì)")
        return text[:2000]  # ìµœëŒ€ 2000ìë¡œ ì œí•œ
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        return ''
    except Exception as e:
        print(f"âŒ ë³¸ë¬¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ''

# 3. í…ìŠ¤íŠ¸ ìš”ì•½ (TextRank)
def summarize_text(text, ratio=0.3):
    """í…ìŠ¤íŠ¸ ìš”ì•½ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
    if not text or len(text.strip()) < 100:
        print("âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ìš”ì•½í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return text[:200] if text else ''
    
    try:
        # ë°©ë²• 1: gensim ì‚¬ìš©
        from gensim.summarization import summarize
        summary = summarize(text, ratio=ratio)
        if summary:
            print(f"âœ… gensimìœ¼ë¡œ ìš”ì•½ ì™„ë£Œ ({len(summary)} ë¬¸ì)")
            return summary
    except ImportError:
        print("âš ï¸ gensimì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ gensim ìš”ì•½ ì˜¤ë¥˜: {e}")
    
    try:
        # ë°©ë²• 2: sumy ì‚¬ìš© (ëŒ€ì•ˆ)
        from sumy.parsers.plaintext import PlaintextParser
        from sumy.nlp.tokenizers import Tokenizer
        from sumy.summarizers.text_rank import TextRankSummarizer
        
        parser = PlaintextParser.from_string(text, Tokenizer("korean"))
        summarizer = TextRankSummarizer()
        sentences = summarizer(parser.document, 3)  # 3ë¬¸ì¥ ìš”ì•½
        summary = ' '.join([str(sentence) for sentence in sentences])
        
        if summary:
            print(f"âœ… sumyë¡œ ìš”ì•½ ì™„ë£Œ ({len(summary)} ë¬¸ì)")
            return summary
    except ImportError:
        print("âš ï¸ sumyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ sumy ìš”ì•½ ì˜¤ë¥˜: {e}")
    
    # ë°©ë²• 3: ê°„ë‹¨í•œ ë¬¸ì¥ ì¶”ì¶œ (ë°±ì—…)
    try:
        sentences = text.split('.')
        # ê¸¸ì´ê°€ ì ë‹¹í•œ ë¬¸ì¥ë“¤ ì„ íƒ
        good_sentences = [s.strip() for s in sentences if 20 < len(s.strip()) < 200]
        
        if good_sentences:
            # ì²˜ìŒ 2-3ê°œ ë¬¸ì¥ ì„ íƒ
            summary = '. '.join(good_sentences[:3]) + '.'
            print(f"âœ… ê°„ë‹¨ ìš”ì•½ ì™„ë£Œ ({len(summary)} ë¬¸ì)")
            return summary
        else:
            # ì›ë³¸ í…ìŠ¤íŠ¸ì˜ ì²˜ìŒ ë¶€ë¶„ ë°˜í™˜
            summary = text[:300] + '...' if len(text) > 300 else text
            print(f"âœ… í…ìŠ¤íŠ¸ ì¼ë¶€ ë°˜í™˜ ({len(summary)} ë¬¸ì)")
            return summary
            
    except Exception as e:
        print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
        return text[:200] if text else ''

# 4. í‚¤ì›Œë“œ ì¶”ì¶œ (KeyBERT)
def extract_keywords(text, top_n=5):
    """í‚¤ì›Œë“œ ì¶”ì¶œ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
    if not text or len(text.strip()) < 50:
        print("âš ï¸ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return []
    
    try:
        # ë°©ë²• 1: KeyBERT ì‚¬ìš©
        from keybert import KeyBERT
        kw_model = KeyBERT()
        keywords = kw_model.extract_keywords(text, top_n=top_n, stop_words='english')
        result = [kw[0] for kw in keywords]
        print(f"âœ… KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {result}")
        return result
    except ImportError:
        print("âš ï¸ KeyBERTê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
    
    try:
        # ë°©ë²• 2: konlpy ì‚¬ìš© (í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„)
        from konlpy.tag import Okt
        from collections import Counter
        import re
        
        okt = Okt()
        # í•œêµ­ì–´ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬ë§Œ ì¶”ì¶œ
        nouns = okt.nouns(text)
        # ê¸¸ì´ê°€ 2 ì´ìƒì¸ ëª…ì‚¬ë§Œ ì„ íƒ
        filtered_nouns = [noun for noun in nouns if len(noun) >= 2]
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        noun_counts = Counter(filtered_nouns)
        # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        keywords = [word for word, count in noun_counts.most_common(top_n)]
        print(f"âœ… konlpyë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords}")
        return keywords
    except ImportError:
        print("âš ï¸ konlpyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ konlpy í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
    
    try:
        # ë°©ë²• 3: ê°„ë‹¨í•œ ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ (ë°±ì—…)
        import re
        from collections import Counter
        
        # í•œê¸€ ë‹¨ì–´ë§Œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        korean_words = re.findall(r'[ê°€-í£]{2,}', text)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = ['ê²ƒì´', 'ìˆëŠ”', 'í•˜ëŠ”', 'ë˜ëŠ”', 'ê°™ì€', 'ë§ì€', 'ì´ëŸ°', 'ê·¸ëŸ°', 'ì €ëŸ°', 
                    'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ë•Œë¬¸', 'í†µí•´', 'ìœ„í•´',
                    'ëŒ€í•œ', 'ê´€ë ¨', 'ê²½ìš°', 'ë•Œë¬¸ì—', 'ì´í›„', 'ì´ì „', 'í˜„ì¬', 'ì˜¤ëŠ˜', 'ì–´ì œ']
        
        filtered_words = [word for word in korean_words if word not in stopwords]
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        word_counts = Counter(filtered_words)
        keywords = [word for word, count in word_counts.most_common(top_n)]
        
        print(f"âœ… ê°„ë‹¨ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords}")
        return keywords
        
    except Exception as e:
        print(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []

# 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
import pandas as pd
import time
import os

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("ğŸ“° ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("="*60)
    
    if not articles:
        print("âŒ ë¶„ì„í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    results = []
    total_articles = min(len(articles), 5)  # ìµœëŒ€ 5ê°œ ê¸°ì‚¬ ì²˜ë¦¬
    
    print(f"ğŸ“Š ì´ {total_articles}ê°œ ê¸°ì‚¬ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤...\n")
    
    for i, article in enumerate(articles[:total_articles], 1):
        print(f"\n[{i}/{total_articles}] ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘...")
        print(f"ğŸ“° ì œëª©: {article['title'][:50]}...")
        
        try:
            title = article['title']
            link = article['link']
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            text = extract_article_text(link)
            if not text:
                print("âš ï¸ ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            # ìš”ì•½ ìƒì„±
            print("ğŸ“ ìš”ì•½ ìƒì„± ì¤‘...")
            summary = summarize_text(text)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            print("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            keywords = extract_keywords(text)
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'title': title,
                'link': link,
                'text_length': len(text),
                'summary': summary if summary else 'ìš”ì•½ ìƒì„± ì‹¤íŒ¨',
                'keywords': ', '.join(keywords) if keywords else 'í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨'
            }
            
            results.append(result)
            print(f"âœ… ê¸°ì‚¬ {i} ì²˜ë¦¬ ì™„ë£Œ")
            
            # ì„œë²„ ë¶€ë‹´ ì¤„ì´ê¸°
            if i < total_articles:
                print("â³ 1ì´ˆ ëŒ€ê¸° ì¤‘...")
                time.sleep(1)
                
        except Exception as e:
            print(f"âŒ ê¸°ì‚¬ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    # ê²°ê³¼ ì €ì¥
    if results:
        try:
            df = pd.DataFrame(results)
            csv_filename = "digital_healthcare_news.csv"
            df.to_csv(csv_filename, index=False, encoding='utf-8-sig')
            
            print(f"\n" + "="*60)
            print("âœ… ë¶„ì„ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼: {csv_filename}")
            print(f"ğŸ“Š ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜: {len(results)}ê°œ")
            print("="*60)
            
            # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
            print("\nğŸ“‹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
            for i, result in enumerate(results, 1):
                print(f"\n[{i}] {result['title'][:50]}...")
                print(f"    ğŸ“ ìš”ì•½: {result['summary'][:100]}...")
                print(f"    ğŸ·ï¸ í‚¤ì›Œë“œ: {result['keywords']}")
                print(f"    ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {result['text_length']}ì")
            
        except Exception as e:
            print(f"âŒ CSV íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            
            # ë°±ì—…: í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
            try:
                with open("digital_healthcare_news_backup.txt", "w", encoding="utf-8") as f:
                    f.write("ë””ì§€í„¸ í—¬ìŠ¤ì¼€ì–´ ë‰´ìŠ¤ ë¶„ì„ ê²°ê³¼\n")
                    f.write("="*50 + "\n\n")
                    
                    for i, result in enumerate(results, 1):
                        f.write(f"[{i}] {result['title']}\n")
                        f.write(f"ë§í¬: {result['link']}\n")
                        f.write(f"ìš”ì•½: {result['summary']}\n")
                        f.write(f"í‚¤ì›Œë“œ: {result['keywords']}\n")
                        f.write("-" * 50 + "\n\n")
                
                print("ğŸ’¾ ë°±ì—… íŒŒì¼ë¡œ ì €ì¥ë¨: digital_healthcare_news_backup.txt")
                
            except Exception as backup_error:
                print(f"âŒ ë°±ì—… íŒŒì¼ ì €ì¥ë„ ì‹¤íŒ¨: {backup_error}")
    else:
        print("\nâŒ ì²˜ë¦¬ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")

# í”„ë¡œê·¸ë¨ ì‹¤í–‰
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        print("ğŸ”§ ë¬¸ì œê°€ ì§€ì†ë˜ë©´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:")
        print("   pip install requests beautifulsoup4 pandas gensim keybert konlpy sumy")
    finally:
        print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
