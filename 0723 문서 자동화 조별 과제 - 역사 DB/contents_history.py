import requests
from bs4 import BeautifulSoup
import csv
import time
import logging
import os
import sys
from urllib.parse import urljoin

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

BASE_URL = "https://contents.history.go.kr/front/contents/search/results.do"
OUTPUT_CSV = "history_contents.csv"

def fetch_page(query="", page=1, max_retries=3):
    """í˜ì´ì§€ë¥¼ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    params = {
        "query": query,
        "pageIndex": page,
        "pageUnit": 10,
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    for attempt in range(max_retries):
        try:
            logger.info(f"í˜ì´ì§€ {page} ìš”ì²­ ì‹œë„ {attempt + 1}/{max_retries}")
            
            session = requests.Session()
            session.headers.update(headers)
            
            resp = session.get(BASE_URL, params=params, timeout=15)
            resp.raise_for_status()
            
            if resp.status_code == 200:
                logger.info(f"í˜ì´ì§€ {page} ì„±ê³µì ìœ¼ë¡œ ê°€ì ¸ì˜´")
                return resp.text
            else:
                logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ìƒíƒœ ì½”ë“œ: {resp.status_code}")
                
        except requests.exceptions.Timeout:
            logger.warning(f"íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ (í˜ì´ì§€ {page}, ì‹œë„ {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                
        except requests.exceptions.ConnectionError:
            logger.warning(f"ì—°ê²° ì˜¤ë¥˜ (í˜ì´ì§€ {page}, ì‹œë„ {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                
        except requests.exceptions.HTTPError as e:
            logger.error(f"HTTP ì˜¤ë¥˜ (í˜ì´ì§€ {page}): {e}")
            if e.response.status_code in [404, 403]:
                logger.error(f"í˜ì´ì§€ ì ‘ê·¼ ë¶ˆê°€: {e.response.status_code}")
                break
            elif attempt < max_retries - 1:
                time.sleep(2 ** attempt)
                
        except requests.exceptions.RequestException as e:
            logger.error(f"ìš”ì²­ ì˜¤ë¥˜ (í˜ì´ì§€ {page}, ì‹œë„ {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
        
        except Exception as e:
            logger.error(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ (í˜ì´ì§€ {page}): {e}")
            break
    
    logger.error(f"í˜ì´ì§€ {page} ëª¨ë“  ì‹œë„ ì‹¤íŒ¨")
    return None

def parse_items(html):
    """HTMLì—ì„œ ì•„ì´í…œë“¤ì„ ì•ˆì „í•˜ê²Œ íŒŒì‹±í•©ë‹ˆë‹¤."""
    if not html:
        logger.warning("HTMLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []
    
    try:
        soup = BeautifulSoup(html, "html.parser")
        items = []
        
        # ë‹¤ì–‘í•œ ì„ íƒì íŒ¨í„´ ì‹œë„
        card_selectors = [
            "div.cards > div.card",
            "div.card",
            ".card",
            "[class*='card']"
        ]
        
        cards = []
        for selector in card_selectors:
            cards = soup.select(selector)
            if cards:
                logger.info(f"ì¹´ë“œ ìš”ì†Œ {len(cards)}ê°œ ë°œê²¬ (ì„ íƒì: {selector})")
                break
        
        if not cards:
            logger.warning("ì¹´ë“œ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        for i, div in enumerate(cards):
            try:
                # ì œëª© ì¶”ì¶œ
                title_selectors = ["h5.card-title", ".card-title", "h5", ".title"]
                title = ""
                for selector in title_selectors:
                    title_elem = div.select_one(selector)
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        break
                
                if not title:
                    logger.warning(f"ì¹´ë“œ {i+1}: ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                # ë§í¬ ì¶”ì¶œ
                link_elem = div.select_one("a")
                if not link_elem or not link_elem.get("href"):
                    logger.warning(f"ì¹´ë“œ {i+1}: ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                link = link_elem["href"]
                if not link.startswith("http"):
                    link = urljoin("https://contents.history.go.kr", link)
                
                # ë‚ ì§œ ì¶”ì¶œ
                date_selectors = ["p.card-date", ".card-date", ".date"]
                date = ""
                for selector in date_selectors:
                    date_elem = div.select_one(selector)
                    if date_elem:
                        date = date_elem.get_text(strip=True)
                        break
                
                # ìš”ì•½ ì¶”ì¶œ
                summary_selectors = ["p.card-text", ".card-text", ".summary", "p"]
                summary = ""
                for selector in summary_selectors:
                    summary_elem = div.select_one(selector)
                    if summary_elem:
                        summary = summary_elem.get_text(strip=True)
                        break
                
                items.append({
                    "title": title,
                    "date": date if date else "ë‚ ì§œ ì—†ìŒ",
                    "summary": summary if summary else "ìš”ì•½ ì—†ìŒ",
                    "url": link
                })
                
                logger.debug(f"ì¹´ë“œ {i+1} íŒŒì‹± ì™„ë£Œ: {title[:30]}...")
                
            except Exception as e:
                logger.warning(f"ì¹´ë“œ {i+1} íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        logger.info(f"ì´ {len(items)}ê°œ ì•„ì´í…œ íŒŒì‹± ì™„ë£Œ")
        return items
        
    except Exception as e:
        logger.error(f"HTML íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        return []

def crawl_all(query="", max_pages=5, delay=1):
    """ëª¨ë“  í˜ì´ì§€ë¥¼ ì•ˆì „í•˜ê²Œ í¬ë¡¤ë§í•©ë‹ˆë‹¤."""
    all_items = []
    consecutive_failures = 0
    max_consecutive_failures = 3
    
    logger.info(f"í¬ë¡¤ë§ ì‹œì‘ - ê²€ìƒ‰ì–´: '{query}', ìµœëŒ€ í˜ì´ì§€: {max_pages}")
    
    for page in range(1, max_pages + 1):
        try:
            logger.info(f"í¬ë¡¤ë§ ì¤‘ - í˜ì´ì§€ {page}/{max_pages}")
            
            html = fetch_page(query=query, page=page)
            
            if not html:
                consecutive_failures += 1
                logger.warning(f"í˜ì´ì§€ {page} HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ì—°ì† ì‹¤íŒ¨: {consecutive_failures})")
                
                if consecutive_failures >= max_consecutive_failures:
                    logger.error(f"ì—°ì† {max_consecutive_failures}íšŒ ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break
                
                # ì‹¤íŒ¨ ì‹œ ë” ê¸´ ëŒ€ê¸°
                time.sleep(delay * 2)
                continue
            
            items = parse_items(html)
            
            if not items:
                consecutive_failures += 1
                logger.warning(f"í˜ì´ì§€ {page} ì•„ì´í…œ íŒŒì‹± ì‹¤íŒ¨ (ì—°ì† ì‹¤íŒ¨: {consecutive_failures})")
                
                if consecutive_failures >= max_consecutive_failures:
                    logger.error(f"ì—°ì† {max_consecutive_failures}íšŒ ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break
                
                # ë¹ˆ í˜ì´ì§€ê°€ ë‚˜ì˜¤ë©´ ë” ì´ìƒ í˜ì´ì§€ê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                if page > 1:
                    logger.info("ë¹ˆ í˜ì´ì§€ ë°œê²¬ - í¬ë¡¤ë§ ì™„ë£Œ")
                    break
                
                time.sleep(delay)
                continue
            
            # ì„±ê³µ ì‹œ ì—°ì† ì‹¤íŒ¨ ì¹´ìš´í„° ë¦¬ì…‹
            consecutive_failures = 0
            all_items.extend(items)
            
            logger.info(f"í˜ì´ì§€ {page} ì™„ë£Œ - {len(items)}ê°œ ì•„ì´í…œ ìˆ˜ì§‘ (ì´ {len(all_items)}ê°œ)")
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            if delay > 0:
                time.sleep(delay)
                
        except KeyboardInterrupt:
            logger.info("ì‚¬ìš©ìì— ì˜í•´ í¬ë¡¤ë§ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
            break
            
        except Exception as e:
            consecutive_failures += 1
            logger.error(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            
            if consecutive_failures >= max_consecutive_failures:
                logger.error(f"ì—°ì† {max_consecutive_failures}íšŒ ì‹¤íŒ¨ë¡œ í¬ë¡¤ë§ ì¤‘ë‹¨")
                break
            
            time.sleep(delay * 2)
            continue
    
    logger.info(f"í¬ë¡¤ë§ ì™„ë£Œ - ì´ {len(all_items)}ê°œ ì•„ì´í…œ ìˆ˜ì§‘")
    return all_items

def save_csv(items, filename):
    """CSV íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì €ì¥í•©ë‹ˆë‹¤."""
    if not items:
        logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    try:
        # ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
        directory = os.path.dirname(filename)
        if directory and not os.path.exists(directory):
            os.makedirs(directory)
            logger.info(f"ë””ë ‰í† ë¦¬ ìƒì„±: {directory}")
        
        # ë°±ì—… íŒŒì¼ ìƒì„± (ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
        if os.path.exists(filename):
            backup_filename = f"{filename}.backup"
            try:
                os.rename(filename, backup_filename)
                logger.info(f"ê¸°ì¡´ íŒŒì¼ ë°±ì—…: {backup_filename}")
            except Exception as e:
                logger.warning(f"ë°±ì—… íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {e}")
        
        keys = ["title", "date", "summary", "url"]
        
        # ë°ì´í„° ê²€ì¦
        valid_items = []
        for i, item in enumerate(items):
            try:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                if not item.get("title"):
                    logger.warning(f"ì•„ì´í…œ {i+1}: ì œëª©ì´ ì—†ì–´ ê±´ë„ˆëœ€")
                    continue
                
                if not item.get("url"):
                    logger.warning(f"ì•„ì´í…œ {i+1}: URLì´ ì—†ì–´ ê±´ë„ˆëœ€")
                    continue
                
                # ë°ì´í„° ì •ë¦¬
                clean_item = {}
                for key in keys:
                    value = item.get(key, "")
                    # íŠ¹ìˆ˜ ë¬¸ì ë° ì¤„ë°”ê¿ˆ ì •ë¦¬
                    if isinstance(value, str):
                        value = value.replace('\n', ' ').replace('\r', ' ').strip()
                        # CSVì—ì„œ ë¬¸ì œê°€ ë  ìˆ˜ ìˆëŠ” ë¬¸ì ì²˜ë¦¬
                        value = value.replace('"', '""')
                    clean_item[key] = value
                
                valid_items.append(clean_item)
                
            except Exception as e:
                logger.warning(f"ì•„ì´í…œ {i+1} ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        if not valid_items:
            logger.error("ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # CSV íŒŒì¼ ì €ì¥
        with open(filename, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=keys)
            writer.writeheader()
            writer.writerows(valid_items)
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(filename)
        logger.info(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {filename}")
        logger.info(f"ğŸ“Š ì €ì¥ëœ ë°ì´í„°: {len(valid_items)}ê°œ ì•„ì´í…œ")
        logger.info(f"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:,} bytes")
        
        # ë°±ì—… íŒŒì¼ ì •ë¦¬
        backup_filename = f"{filename}.backup"
        if os.path.exists(backup_filename):
            try:
                os.remove(backup_filename)
                logger.info("ë°±ì—… íŒŒì¼ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ë°±ì—… íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        return True
        
    except PermissionError:
        logger.error(f"íŒŒì¼ ì“°ê¸° ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤: {filename}")
        return False
        
    except OSError as e:
        logger.error(f"íŒŒì¼ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
        return False
        
    except Exception as e:
        logger.error(f"CSV ì €ì¥ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
        return False

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        logger.info("í•œêµ­ì‚¬ ì½˜í…ì¸  í¬ë¡¤ëŸ¬ ì‹œì‘")
        logger.info("=" * 50)
        
        # ì‚¬ìš©ì ì„¤ì •
        search_query = input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì—”í„°: ì „ì²´ ê²€ìƒ‰): ").strip()
        
        try:
            max_pages = int(input("ìµœëŒ€ í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 10): ") or "10")
            if max_pages <= 0:
                max_pages = 10
        except ValueError:
            max_pages = 10
            logger.warning("ì˜ëª»ëœ í˜ì´ì§€ ìˆ˜ ì…ë ¥, ê¸°ë³¸ê°’ 10 ì‚¬ìš©")
        
        try:
            delay = float(input("í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)ì„ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 0.5): ") or "0.5")
            if delay < 0:
                delay = 0.5
        except ValueError:
            delay = 0.5
            logger.warning("ì˜ëª»ëœ ëŒ€ê¸° ì‹œê°„ ì…ë ¥, ê¸°ë³¸ê°’ 0.5ì´ˆ ì‚¬ìš©")
        
        logger.info(f"ì„¤ì • - ê²€ìƒ‰ì–´: '{search_query}', ìµœëŒ€ í˜ì´ì§€: {max_pages}, ëŒ€ê¸° ì‹œê°„: {delay}ì´ˆ")
        
        # í¬ë¡¤ë§ ì‹¤í–‰
        data = crawl_all(query=search_query, max_pages=max_pages, delay=delay)
        
        if not data:
            logger.error("í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # CSV ì €ì¥
        success = save_csv(data, OUTPUT_CSV)
        
        if success:
            logger.info("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
            logger.info(f"ğŸ“ ì €ì¥ëœ íŒŒì¼: {OUTPUT_CSV}")
        else:
            logger.error("âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        logger.info("â¹ï¸ ì‚¬ìš©ìì— ì˜í•´ í”„ë¡œê·¸ë¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
