import pdfplumber
import re
import pandas as pd
import os
import sys

# ì„ íƒì  import (ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ì–´ë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡)
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    SKLEARN_AVAILABLE = True
except ImportError:
    print("âš ï¸ scikit-learnì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TF-IDF ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ ì„¤ì¹˜ ë°©ë²•: pip install scikit-learn")
    SKLEARN_AVAILABLE = False

try:
    from keybert import KeyBERT
    KEYBERT_AVAILABLE = True
except ImportError:
    print("âš ï¸ KeyBERTê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. KeyBERT ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ ì„¤ì¹˜ ë°©ë²•: pip install keybert")
    KEYBERT_AVAILABLE = False

try:
    from konlpy.tag import Okt
    KONLPY_AVAILABLE = True
except ImportError:
    print("âš ï¸ KoNLPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ğŸ’¡ ì„¤ì¹˜ ë°©ë²•: pip install konlpy")
    KONLPY_AVAILABLE = False

# 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def extract_text_from_pdf(pdf_path):
    """PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
    
    try:
        full_text = ""
        with pdfplumber.open(pdf_path) as pdf:
            print(f"ğŸ“„ PDF í˜ì´ì§€ ìˆ˜: {len(pdf.pages)}")
            
            for i, page in enumerate(pdf.pages):
                try:
                    page_text = page.extract_text()
                    if page_text:
                        full_text += page_text + "\n"
                    else:
                        print(f"âš ï¸ {i+1}í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    print(f"âš ï¸ {i+1}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        if not full_text.strip():
            raise ValueError("âŒ PDFì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        print(f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ (ì´ {len(full_text)} ë¬¸ì)")
        return full_text
        
    except Exception as e:
        raise Exception(f"âŒ PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# 2. ë‹¨ì› ê¸°ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 
def split_by_chapter(text):
    """í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì›ë³„ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    if not text or not text.strip():
        print("âš ï¸ ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return [], []
    
    # ë‹¤ì–‘í•œ íŒ¨í„´ ì‹œë„
    patterns = [
        r"\n\d{2} [^\n]+",  # ê¸°ë³¸ íŒ¨í„´: "01 ë‹¨ì›ëª…"
        r"\n\d{1,2}\. [^\n]+",  # "1. ë‹¨ì›ëª…"
        r"\nì œ\d{1,2}ì¥ [^\n]+",  # "ì œ1ì¥ ë‹¨ì›ëª…"
        r"\në‹¨ì› \d{1,2} [^\n]+",  # "ë‹¨ì› 1 ë‹¨ì›ëª…"
    ]
    
    for pattern in patterns:
        headers = re.findall(pattern, text)
        if headers:
            print(f"âœ… íŒ¨í„´ '{pattern}'ìœ¼ë¡œ {len(headers)}ê°œ ë‹¨ì› ë°œê²¬")
            chapters = re.split(pattern, text)
            # ì²« ë²ˆì§¸ ìš”ì†ŒëŠ” ë³´í†µ ë¹ˆ ë¬¸ìì—´ì´ë¯€ë¡œ ì œê±°
            if chapters and not chapters[0].strip():
                chapters = chapters[1:]
            return headers, chapters
    
    print("âš ï¸ ë‹¨ì› íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë‹¨ì›ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    return ["ì „ì²´ ë‚´ìš©"], [text]

# 3. ìš”ì•½ ë° í‚¤ì›Œë“œ ì¶”ì¶œ (TF-IDF)
def extract_keywords_tfidf(docs, top_k=5):
    """TF-IDFë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not SKLEARN_AVAILABLE:
        print("âš ï¸ scikit-learnì´ ì—†ì–´ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return [["í‚¤ì›Œë“œ ì¶”ì¶œ ë¶ˆê°€"] for _ in docs]
    
    if not docs or all(not doc.strip() for doc in docs):
        print("âš ï¸ ë¬¸ì„œê°€ ë¹„ì–´ìˆì–´ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return [["ë‚´ìš© ì—†ìŒ"] for _ in docs]
    
    try:
        # í•œêµ­ì–´ ë¶ˆìš©ì–´ í™•ì¥
        korean_stopwords = [
            "ê²ƒ", "ìˆ˜", "ë“±", "ì´", "ê·¸", "ì˜", "ê°€", "ì„", "ë¥¼", "ì—", "ì™€", "ê³¼", 
            "ë„", "ëŠ”", "ì€", "í•œ", "í•˜ë‹¤", "ìˆë‹¤", "ë˜ë‹¤", "ê°™ë‹¤", "ë•Œë¬¸", "í†µí•´",
            "ìœ„í•´", "ëŒ€í•œ", "ê´€í•œ", "ë”°ë¼", "ìœ„í•œ", "ëŒ€í•´", "ë…„", "ì›”", "ì¼"
        ]
        
        tfidf = TfidfVectorizer(
            max_features=500, 
            stop_words=korean_stopwords,
            min_df=1,  # ìµœì†Œ ë¬¸ì„œ ë¹ˆë„
            max_df=0.8,  # ìµœëŒ€ ë¬¸ì„œ ë¹ˆë„
            ngram_range=(1, 2)  # 1-2 ë‹¨ì–´ ì¡°í•©
        )
        
        tfidf_matrix = tfidf.fit_transform(docs)
        feature_names = tfidf.get_feature_names_out()
        
        keywords = []
        for i in range(tfidf_matrix.shape[0]):
            row = tfidf_matrix[i].toarray().flatten()
            top_indices = row.argsort()[-top_k:][::-1]
            top_keywords = [feature_names[idx] for idx in top_indices if row[idx] > 0]
            
            if not top_keywords:
                top_keywords = ["í‚¤ì›Œë“œ ì—†ìŒ"]
            
            keywords.append(top_keywords)
        
        print(f"âœ… TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        return keywords
        
    except Exception as e:
        print(f"âš ï¸ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return [["ì˜¤ë¥˜ ë°œìƒ"] for _ in docs]

# 4. KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ (ì„ íƒ ì‚¬í•­)
def extract_keywords_keybert(docs, top_k=5):
    """KeyBERTë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    if not KEYBERT_AVAILABLE:
        print("âš ï¸ KeyBERTê°€ ì—†ì–´ KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return [["KeyBERT ë¶ˆê°€"] for _ in docs]
    
    try:
        kw_model = KeyBERT()
        korean_stopwords = ["ê²ƒ", "ìˆ˜", "ë“±", "ì´", "ê·¸", "ì˜", "ê°€"]
        
        keywords = []
        for doc in docs:
            if not doc.strip():
                keywords.append(["ë‚´ìš© ì—†ìŒ"])
                continue
                
            try:
                kw_list = kw_model.extract_keywords(
                    doc, 
                    top_n=top_k, 
                    stop_words=korean_stopwords,
                    use_mmr=True,  # ë‹¤ì–‘ì„± ì¦ê°€
                    diversity=0.5
                )
                extracted = [kw[0] for kw in kw_list] if kw_list else ["í‚¤ì›Œë“œ ì—†ìŒ"]
                keywords.append(extracted)
            except Exception as e:
                print(f"âš ï¸ KeyBERT ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                keywords.append(["ì˜¤ë¥˜ ë°œìƒ"])
        
        print(f"âœ… KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
        return keywords
        
    except Exception as e:
        print(f"âš ï¸ KeyBERT í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        return [["ì˜¤ë¥˜ ë°œìƒ"] for _ in docs]

# 5. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ“š í•œêµ­ì‚¬ PDF ë¶„ì„ ì‹œì‘")
    print("=" * 50)
    
    # PDF íŒŒì¼ ê²½ë¡œ ì„¤ì •
    pdf_path = "2026 ìˆ˜ëŠ¥íŠ¹ê°•_ í•œêµ­ì‚¬.pdf"
    
    try:
        # 1. PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        print("1ï¸âƒ£ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘...")
        text = extract_text_from_pdf(pdf_path)
        
        # 2. ë‹¨ì›ë³„ ë¶„í• 
        print("2ï¸âƒ£ ë‹¨ì›ë³„ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
        headers, chapters = split_by_chapter(text)
        
        if not headers or not chapters:
            print("âŒ ë‹¨ì›ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f"âœ… {len(headers)}ê°œ ë‹¨ì› ë°œê²¬")
        
        # 3. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        print("3ï¸âƒ£ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì¤‘...")
        chapters_cleaned = []
        for i, chapter in enumerate(chapters):
            if chapter and chapter.strip():
                cleaned = re.sub(r"\s+", " ", chapter.strip())
                chapters_cleaned.append(cleaned)
            else:
                print(f"âš ï¸ {i+1}ë²ˆì§¸ ë‹¨ì›ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                chapters_cleaned.append("ë‚´ìš© ì—†ìŒ")
        
        # 4. í‚¤ì›Œë“œ ì¶”ì¶œ
        print("4ï¸âƒ£ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
        keywords = extract_keywords_tfidf(chapters_cleaned)
        
        # 5. ê²°ê³¼ ì •ë¦¬
        print("5ï¸âƒ£ ê²°ê³¼ ì •ë¦¬ ì¤‘...")
        
        # ë°ì´í„° ê¸¸ì´ ë§ì¶”ê¸°
        min_length = min(len(headers), len(chapters_cleaned), len(keywords))
        
        df = pd.DataFrame({
            "ë‹¨ì› ì œëª©": [h.strip() for h in headers[:min_length]],
            "ìš”ì•½ (100ì ì´ë‚´)": [c[:100] + "..." if len(c) > 100 else c 
                              for c in chapters_cleaned[:min_length]],
            "í‚¤ì›Œë“œ": keywords[:min_length]
        })
        
        # 6. CSV ì €ì¥
        output_file = "í•œêµ­ì‚¬_ë‹¨ì›ìš”ì•½_í‚¤ì›Œë“œ.csv"
        df.to_csv(output_file, index=False, encoding="utf-8-sig")
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ!")
        print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼: {output_file}")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ ë‹¨ì› ìˆ˜: {len(df)}")
        
        # ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        print("\nğŸ“‹ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:")
        print(df.head())
        
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {e}")
        print("ğŸ’¡ PDF íŒŒì¼ì´ í˜„ì¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

# ì‹¤í–‰
if __name__ == "__main__":
    main()
